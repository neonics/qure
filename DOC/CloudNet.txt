= Distributed Architecture =
# 2013-09-30?


Setup with 2 instances on a LAN:

The internet gateway is configured to forward certain ports to a fixed ip.
Running two instances on a LAN then will have only one of them respond to
packets for the IP. The IP is (pre)configured in the os.

Initialisation routine on a LAN then involves pinging the fixed IP to
determine whether it is available. If so, the NIC is configured with the IP.
If not, another IP is chosen.

The machine that is not using the gateway's forward IP regularly pings the IP,
and when there is no response, reconfigures itself with the IP.

== 'Master/Slave' ==
[=DMZ]

In this setup, only one instance on the LAN has obtained the public IP, and is
called the 'master' or DMZ node. It is aware of other instances using a multicast
protocol, and could forward packets to load balance within a LAN.
See also [MigratingTCP].


	WAN: .cloud

	subdomain: cloud.neonics.com
	subdomain DNS: cloudns.neonics.com (DYNA record).

The OS currently implements a DNS proxy for cloud.neonics.com,
returning the address for cloudns.neonics.com, which is the public ip
of the (so far) only cluster running the OS.

The model can serve other domains aswell, and illustrates how updates
can propagate.

All cloud machines will check the main DNS server and register themselves
by using the DNS update mechanism. 




== Deployment ==

It is envisioned that QuRe will be deployed on private (non-corporate)
sites, using modems connected through fiber, the telephone network,
or the media network. The targeted domain then consists of consumers,
which may not have the expertise of advanced network configuration,
or running a server. The gateway then must be assumed to be the most
limiting version. Normally these have a built-in DHCP server.
It is required that these gateways offer portforwarding or DMZ.

There is a number of protocols for dynamic gateway configuration such as
portforwarding, though this is not commonly supported with low-end modems.



Communication behind a gateway (LAN)
------------------------------------
Since a single gateway only can forward packets for a particular port to one IP,
when multiple cloud machines are present behind the gateway, one must be chosen
to handle the requests.

There are a number of options for a protocol that allows the machines to
be aware of each other.
First, some sort of broadcast is necessary:
* the broadcast IP for the subnet
* multicast (224.0.0.0/4)

Second, a protocol is needed:
* multicast group membership: IGMP
* netbios
* DNS

A particular multicast group number can be reserved for cloud based services,
where membership of the group, made known by group reports, indicates
the presence of cloud machines.
[update: this is implemented; the reserved IP is ''224.0.0.123'']

Netbios is proprietary and shall not be used.

DNS can be used over multicast.
SSDP is HTTP over multicast.

For IPv6, there is NDP (neighbour discovery protocol), which is ICMPv6,
used for:
* router sollicitation/advertisement
* neighbour sollicitation/advertisement
* redirect


Router Configuration
--------------------
Option 1: fixed IP in LAN subnet outside of DHCP IP range.
Downside: there is no way for the machine to discover which IP this might be,
and thus would be a preconfigured default which would require configuration
in the virtual machine (not zeroconf).

Option 2: fixed multicast IP in some unused range.
Downside: the network will be flooded with all incoming communication. This
is desirable so that it reaches all machines, however, this would nullify
switch optimization which would reduce network traffic once the machine IP's
MAC is known. In most cases however, a router will have WiFi and thus will
broadcast anyway.



Basic operation behind a firewall/gateway
-----------------------------------------
Assumptions:
* All machines know the IP to which the service ports are forwarded;
* All machines are aware of each other through the LAN awareness protcol
* One machine is elected to handle requests, which is known by all machines.
[!edit:2014-07-12]
* All machines monitor LAN activity and are aware of DMZ requests
Tested the router, and it will not accept a broadcast or multicast DMZ LAN IP;
the LAN DMZ node could broadcast requests to particular multicast addresses,
membership of which indicates the ability to handle such requests. 

[!end edit]

Then, all machines monitor the requests, and can take action when the response
is not forthcoming within a timeout period.

It is foreseen that the machines may be shut down at any time, by
terminating or suspending the virtual machine emulator, in which case they
do not broadcast their disappearance.

Further, response times must be as low as possible, say within a second.

This then would require all machines to communicate their presence with each
other, which would flood the network unnecessarily.

When all machines receive all requests, monitoring the presence of a response
from the elected machine then determines whether or not a new machine must
be elected to handle the response.

When a machine detects a timeout in the response time, it can query the
elected machine to see if it is still present. If not, this then serves as
the trigger to elect a new machine to handle requests.


Load balancing
--------------
Multiple machines may handle requests on a socket (remote ip+port) bases
so that requests coming from the same client will be handled by the same
service. Unfortunately the use of HTTP is widespread, which uses many
different connections for a single session, and thus uses different ports.
Furthermore, some remote IP's are gateways themselves, such as in use
by universities, which may house many thousands of clients, all appearing
as the same IP.

---------------------------------------------------------------------

ACPI shutdown detect
--------------------
On shutdown detect, a message can be broadcast.

ACPI event detection is implemented, but VMWare Player does not send such
events (even when power options are manually configured as 'soft').

Shared DMZ
----------
Each vm can implement multiple virtual devices. It can use 192.168.1.11
as the shared DMZ alongside a DHCP address.
A second MAC must be made - possibly a virtual NIC, for the 2nd IP
address (like eth0:0).

Running multiple VM's: keep each other informed of who is handling DMZ.
Regular peer-to-peer messages to keep aware of DMZ IP assigned to VM.
On detection of the DMZ vm having gone offline, another takes over.

Obtaining the DMZ IP is done by updating the NIC IP with the configured
DMZ IP. Nodes recognize each other by MAC. For routing purposes however,
when de-configuring the nic IP, it unsubscribes from it's MCAST
memberships, sends an UNARP for it's old DHCP address, and resubscribes
to the cloud MCAST address.
It could even notify the DHCP server.


Host Daemon VM Manager
----------------------
A host daemon can participate in the protocol. It can then configure
all VM's using network messages.
It can also start new VM's and recycle existing ones that do not respond.

VMs can be started in 'nogui' mode (''vmrun -T player start file.vmx [no]gui'').


Virtual / Shared IP
-------------------
The DMZ ip will be configured as a virtual service ip.

Each cluster node will be configured over the network to support
the DMZ ip.  [note: at first a single IP, will be made a list later]


Bootstrap Node
---------------
Only a single node needs to be configured, if the other cluster
nodes accept their configuration over the network.


Multicast
---------
All nodes join a group using a GMP (IGMP). This makes use of
multicasting (not broadcasting). To determine which number
represents the cluster communication point, a hardcoded/reserved
number can be used. There can perhaps be a broadcast to request
which groups there are. Perhaps the group can be resolved
using multicast DNS.


DHCP
----
Each node can function as a DHCP server, using a custom service number.
It can return the required network configuration, and some field
can be used to indicate that this is information for the cluster.
In this way, each cluster node can do DHCP requests and receive
multiple answers. One will be the localnet, the other will be
the cluster - such as some identifier (the IP need not be used as such),
and the gateway or server ip fields can indicate which cluster node
is taken to be the configuration node.


ZeroConfig Bootstrapping using IP monitoring
--------------------------------------------
The DMZ ip, once configured in the router, will give rise
to public traffic. This traffic will be directed to the DMZ
ip, which the router will ARP.

If there are repeated ARP requests for a certain ID, a node will
answer, claiming the IP, and generating a new MAC different
from it's LAN MAC by which the router knows it. This prevents
caching a temporary MAC with a fixed IP.

In this scheme, the IP is fixed, as it is a service IP configured
in the router. The MAC is temporary, since any cluster node
can take the IP if there is no answer.

A script-expression rule for this then might be:

	auto-answer ARP $LANMASK

which would have any node respond (sensibly, as it knows the IP
it takes is a shared virtual IP and thus may be in use).

[!Best would be to have the router/gateway provide the service IP
in DHCP].


mDNS
----
A fixed addressing scheme - say ''.cloud'' or ''.localcluster''
can provide DNS ''SRV'' records.

The DNS then can serve as a simplified LDAP system, where information
is stored in nodes - an object hierarchy.


Custom Protocol
===============
Requirements:

1) Maintain Cluster Knowledge
 a) broadcast on boot [status:COMPLETE]
 b) multicast PING
	
	Multicast (notes from cisco)
	---------
	free IP range: 239.0.0.0-239.255.255.255 (239/8)
	(prohibited on internet/ttl=1)
	dynamic allocation: SDR, Multicast Address Set Claim (MASC), SSM; MADCAP, SSM 
	
	static: 'GLOP': 233/8: 233.[16bit ASN].[0-255]

	ASN: autonomous system (ISP, content provider).

	SDR - session directory:
	SAP: session announcement protocol
	SDR: well known global mcast say 224.2.127.254.
		IP multicast addresses
		time session

 c) cluster uptime: session duration.


Shared Objects
--------------
The goal of the Cluster is to keep all data alive. The cluster uptime then
consists of time since the first node booted.  Taking an OO approach, the
cluster maintains objects.  The root object itself is the cluster uptime and
version.  Each time a cluster dies, all of the nodes have powered off.

Each cluster keeps persistent track of boot times.  Cluster nodes might be
identifiable by a unique cluster id; their DHCP IP will generally change.  A
history then, a graph of the number of online nodes, is attempted to be shared
and thus kept consistent.

The cluster history consists of events. The events are numbered.  History is
divided in ages - each age represents the lifetime of a cluster. An age then
indicates when the first node of a cluster boots. 

This information is already available, as the DMZ ip - a shared resource -
is requested, and claimed. A second node will (ARP) request the same IP,
but find a response. The first node then knows it was the first,
the second that it wasn't. Each time a node boots that is the first,
it increments the cluster age.

Within an age, the appearance of a node constitutes an event.

Each node broadcasting it's presence will be answered by a single
node. All nodes receive the broad/multi cast. The nodes have chosen
a function - first responder - and determined which node serves
the role. 

When a node appears that announces a prior age than is current,
it receives the history since.

FIRST IMPLEMENTATION
--------------------
hello protocol: send last age ('date').

OOFS: ?/cluster

receives response. If age received:
	less: the cluster is out of date. The nodes haven't been
		participating in the cluster for a while. This
		node then has more recent information, which
		it shares with the rest to update their history
		events.
	more: the reverse.
Note that it doesnt matter which node sends or receives - in the broadcast,
the packet is handled as a response by the receiving nodes, who
have chosen one to respond to the booted node. This node will send
the cluster age and it's own.

Nodes then keep track of participating in ages - their personal history.

	cluster_history:
		cluster_age:	.long		# array length
		incarnations:	.space 4*?	# [array of cluster_incarnation]

	node_history:
		node_age:	.long		# array length
		eras:		.space 4*	# era ID's in which incarnated.

			An era does not change during any node incarnation,
			because a new era only begins if this one ends,
			which is when there are no nodes incarnated.

	hello_packet:
		era:	.long 0	# cluster age
		age:	.long 0	# node incarnation





Kernel Revision
---------------
Revisions can be counted per branch using git log --pretty=oneline | wc -l.
The cluster era then can be set to start at a particular revision.
The number of reboots during an era then serves as a second revision
number.
From eras prior to the runtime revision counting their reboots yields
an age: the number of tests, and their duration - history: time between
reboots.

	cluster_node_onload:
		mov	edx, [eax + era]
		cmp	edx, KERNEL_REVISION
		jz	1f
		mov	[eax + era], KERNEL_REVISION
		.if ?
		mov	edx, [eax + age]
		mov	[eax + era_start], edx
		.else
		mov	[eax + age], dword ptr 0
		.endif

	1:	ret


	cluster_get_kernel_version:
		mov	edx, [eax + age]
		sub	edx, [eax + era_start]
		mov	eax, [eax + age]
		ret



Crash Detection
===============
# 2013-11-22

All nodes broadcast a keepalive or ping packet every minute. All nodes
keep track of a last_seen in terms of clocks. Every minute after sending
the ping, the last_seen for all nodes is checked to a timeout.

To not flood the network, this is done periodically, and no response
is expected except within a minute. Since each node periodically pings
within the timeout period, this is satisfied.

However, a timeout of a minute means that a node can be down for almost two
before it is determined to be offline.

A node that is in the debugger, and thus has no scheduling, and therefore
no packet handling, is still at least partially operational, and could
broadcast a message indicating it has halted processing, unless the exception
occurred in a critical path.

Vice versa, using the scheme as described in [Alert], minimal reception
might be enabled, such as a kind of 'wake-on-lan' packets which could trigger
a reboot. If the network card is configurable to detect certain kinds of packets,
then the network card interrupt service routine would probably be able to detect
this by simply checking a flag on the incoming packet, and thus trigger
a reboot.

The shared virtual IP, currently hardcoded, needs to be shared in the cluster
data which is maintained. At current only the cluster birthdate is propagated
across the nodes, but this is easily extensible. Once a node detects that a
node is down, it also knows, by checking the ip, whether it is the DMZ node
or not. If it is, it will attempt to claim the shared IP.

Synchronisation and optimal distribution
---------------

Multiple nodes could decide simultaneously to take on the role of DMZ, although
the chance is very small. The distance would need to be at least the delay
between two ARP requests. Therefore, the nodes will communicate their periodic
offset, which will allow all nodes to know the distribution. This offset is
already provided by the arrival time. 

Nodes would optimally shift their periods so as to provide an even distribution.
Two nodes would have each one ping 30 seconds before and after the other.
For three nodes, the distance would be 20 seconds, and so on.

Each node will, after filtering out the unresponsive nodes, take stock of
the number of nodes that have responded within the last minute. It will then
know how much it needs to shift it's phase relative to the others.

The nodes will be sorted according to phase. The node will place itself
initially at zero-point, and determines the distance to the nearest node
on either side. It will then sort the list according to smallest distance.
The starting point will be the node with the smallest distance to the next,
and the longest distance to the previous. Since the nodes are sorted, and
since the period is cyclical, such a starting point then becomes obvious: there
is only one point where the increase in distance decreases.

Since all nodes follow this algorithm, they will all come up with the exact same
list. In the case where two nodes are very close together, and may be perceived
in a different order in some nodes, it is possible that both nodes will adjust
their phase in the same, rather than the opposite, direction. A minimum distance,
such as 5 seconds, will have both nodes shift at non-ideal distance, each say 2
seconds from the average. Thus, in the case where there are two nodes in a cluster,
spaced a few milliseconds apart, one node would shift it's phase forward by 15
seconds, and one backwards by 15 seconds. If they shift in the same direction,
the situation won't change. Therefore, one will shift forward 17 seconds, and
one will shift backward 12 seconds (or vice versa), depending on the order of their
IP address. The node with the lower IP will subtract half the minimum distance
(of 5 seconds), whereas the other will add. In the case where both shift in
the same direction, the distance will now be near the minimum distance.
Another shift will make sure the minimum distance is exceeded. Ofcourse, the
distance of 5 seconds is long enough to provide for proper ordering of ping
reception times in order to determine the direction of phase shift. Ofcourse,
two values might be used instead - a minimum distance, and a deviation from
the ideal shift, possibly dependent upon the number of nodes in the cluster,
or the number of nodes within the minimum distance.

Thus in two cycles, or, two minutes, the nodes will be ideally spaced,
providing for optimum sampling times without flooding the network.
Further, the distance will be far enough apart (unless there are many nodes),
that the node claiming DMZ will achieve a result without overlapping such
an operation of another nearby node.

A node that would claim DMZ knows which node is before it in the list - as
it is the last node it received a ping from - and could communicate
peer-to-peer to query whether it has already assumed DMZ. Ofcourse, this
is already implemented in the gratuitious ARP, which the prior node would
respond to. Further, such gratuitious ARP is broadcast, and thus known by
all nodes.

A node in such a case would receive a gratuitious arp from the previous
node in the list for the DMZ ip, and thus know that the DMZ node has
timed out. It detects this by the MAC address in the ARP packet, as the
node's IP is no longer in use.

	NOTE: this makes it important that nodes be identified by their MAC
	rather than their IP, as IP's change. The DHCP server does attempt
	to maintain the same IP's for the same MAC's, yet, if a node changes
	it's IP to the DMZ IP, the record for the DMZ node will be overwritten
	with a copy of the assuming node. So far, this is not an issue since
	the only information kept on nodes is their role - their IP.



Security Ring
=============

Now that nodes can be (and are) rebooted if they are found to be offline,
it is needful to establish a way for this to be done securely, so that
packet injection in the network will not compromise the system.

A key can be generated and communicated using mcast or even peer-to-peer,
yet this can be snooped. Since a snooper will have all the information
that any node has, something else is needed.

A node joining the cluster for the first time must be authorized. A single
node in a ring of trust is enough. The connecting node will print it's 
fingerprint on the console, and a cluster node is chosen to also print
this and ask whether to allow the node onto the cluster. Once user
intervention has taken place, the cluster is accepted.

The code in ''net.s'' checking for the reboot packet will then have to
check for a signature. This would require it to have access to the cluster
node list to verify the signature. The code therefore will be moved
to the cloudnet code base, and a hook in the network code made available
though a kernel api to register a reboot packet filter.

Local Elevation
---------------
The kernel function registering the reboot packet filter would only be
accessible for certain processes. Since a process does not need to
run at CPL0, it may temporarily ask for elevation merely to execute
the function. The kernel API thus can alert to ask whether to allow
the process access to the function.

To automate this, processes or programs may be signed, and their
signatures added to a list specifying permissions, or, using a group
membership or list of roles.

For now, the kernel may simply check whether the returning address
of the kernel call, aswell as the pointer to the reboot packet filter,
is within the kernel code range in order to establish trust. This is
secure since all non-kernel programs will be loaded in a different
address space, as the kernel is mapped in all user processes.


